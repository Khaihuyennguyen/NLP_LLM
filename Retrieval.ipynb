{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSUfjEfTItvZGdZmoD4DnG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khaihuyennguyen/NLP_LLM/blob/main/Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Retrieval Question Answering\n",
        "\n",
        "## Set up\n"
      ],
      "metadata": {
        "id": "4Q5_QhySgH_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T95ZaINogFf9"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq rich openai==0.27.2 tiktoken wandb langchain unstructured tabulate pdf2image chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "from pathlib import PurePosixPath\n",
        "import tiktoken\n",
        "from getpass import getpass\n",
        "from rich.markdown import Markdown"
      ],
      "metadata": {
        "id": "9eBv2zUcgRd4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "230Tn3iqgp8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getenv(\"OPENAI_API_KEY\") = None\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
        "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
        "print(\"OpenAI API key configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "ZGrr_qN0gayO",
        "outputId": "7ec95e88-d208-41b5-c89b-69325767c3af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-e067089e831f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    os.getenv(\"OPENAI_API_KEY\") = None\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R0qMP2NYFx4",
        "outputId": "a9827c55-eb43-4d76-ffda-7ca019979177"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI key from: https://platform.openai.com/account/api-keys\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "\n",
        "LangChain is a framework for developing application powered by langauge models. We will use some of its features in the code below. Let's start by configuring W&B tracking"
      ],
      "metadata": {
        "id": "Ctr90hnAgthm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need a single line of code to start tracing langchain with W&B\n",
        "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
        "\n",
        "# wandb documentation to configure wandb using env variables\n",
        "# https://docs.wandb.ai/guides/track/advanced/environment-variables\n",
        "# here we are configuring the wandb project name\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = \"llmapps\""
      ],
      "metadata": {
        "id": "A3VcEKc1gizI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing documents\n",
        "\n",
        "We will use a small sample of markdown documents in this notebook. Let's find them and make sure we can stuff them into the prompt. That means we may need to be chnked and not exceed some number of tokens"
      ],
      "metadata": {
        "id": "UNcx6lPoUZkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"text-davinci-003\"\n"
      ],
      "metadata": {
        "id": "bwo-n2dsUYRW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab some sample data,\n",
        "\n",
        "!git clone https://github.com/wandb/edu.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkv4zJrdUp33",
        "outputId": "4f451bef-d623-4332-daff-42d15f0d42a8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'edu' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need to count tokens in the documents , and for that we need the tokenizer\n",
        "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)"
      ],
      "metadata": {
        "id": "esqk0i3bUthM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "def find_md_files(directory):\n",
        "  \"Find all markdown files in a directory and return a LangChain documents\"\n",
        "  dl = DirectoryLoader(directory, \"**/*.md\")\n",
        "  return dl.load()\n",
        "\n",
        "documents = find_md_files('edu/llm-apps-course/docs_sample/')\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9_nWI5VVZg",
        "outputId": "d889efac-41b9-41fb-f73f-8024ee8dec93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The function to count the tokens in each doc\n",
        "def count_tokens(documents):\n",
        "  token_counts = [len(tokenizer.encode(document.page_content)) for document in documents]\n",
        "  return token_counts\n",
        "count_tokens(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ8FoRVyU3_u",
        "outputId": "8afb424c-6698-4226-db7f-53bb6ae4df3a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2135, 395, 763, 310, 665, 1957, 1154, 1199, 2657, 2676, 2330]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use LangChain built in Markdowntextsplitter to split the documents into sections.\n",
        "\n",
        "- We can pass the chunk_size param and avoid lengthy chunks\n",
        "- The chunk_overlap param is usefu so you dont cut sentences randomly. This is less necessary with Markdown"
      ],
      "metadata": {
        "id": "RPSIi4POV6Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "md_text_splitter = MarkdownTextSplitter(chunk_size = 1000)\n",
        "document_sections = md_text_splitter.split_documents(documents)\n",
        "len(document_sections), max(count_tokens(document_sections))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYw__CvdVMbd",
        "outputId": "797ca7d3-5186-48c6-9d28-b907b648899e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88, 438)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let see the first section\n",
        "Markdown(document_sections[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "SdHPfa7FWi3m",
        "outputId": "d640b6c8-0a5d-4c5b-d9a0-597c546deec7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "description: Guide to using Artifacts for dataset versioning                                                       \n",
              "\n",
              "Dataset Versioning                                                                                                 \n",
              "\n",
              "W&B Artifacts help you save and organize machine learning datasets throughout a project's lifecycle.               \n",
              "\n",
              "Common use cases                                                                                                   \n",
              "\n",
              "Version data seamlessly, without interrupting your workflow                                                        \n",
              "\n",
              "Prepackage data splits, like training, validation, and test sets                                                   \n",
              "\n",
              "Iteratively refine datasets, without desynchronizing the team                                                      \n",
              "\n",
              "Juggle multiple datasets, as in fine-tuning and domain adaptation                                                  \n",
              "\n",
              "Visualize & share your data workflow, keeping all your work in one place                                           \n",
              "\n",
              "Flexible tracking and hosting                                                                                      \n",
              "\n",
              "Beyond these common scenarios, you can use core Artifact features to upload, version, alias, compare, and download \n",
              "data, supporting any custom dataset workflow on local or remote file systems, via S3, GCP, or https.               \n",
              "\n",
              "Core Artifacts features                                                                                            \n",
              "\n",
              "W&B Artifacts support dataset versioning through these basic features:                                             \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">description: Guide to using Artifacts for dataset versioning                                                       \n",
              "\n",
              "Dataset Versioning                                                                                                 \n",
              "\n",
              "W&amp;B Artifacts help you save and organize machine learning datasets throughout a project's lifecycle.               \n",
              "\n",
              "Common use cases                                                                                                   \n",
              "\n",
              "Version data seamlessly, without interrupting your workflow                                                        \n",
              "\n",
              "Prepackage data splits, like training, validation, and test sets                                                   \n",
              "\n",
              "Iteratively refine datasets, without desynchronizing the team                                                      \n",
              "\n",
              "Juggle multiple datasets, as in fine-tuning and domain adaptation                                                  \n",
              "\n",
              "Visualize &amp; share your data workflow, keeping all your work in one place                                           \n",
              "\n",
              "Flexible tracking and hosting                                                                                      \n",
              "\n",
              "Beyond these common scenarios, you can use core Artifact features to upload, version, alias, compare, and download \n",
              "data, supporting any custom dataset workflow on local or remote file systems, via S3, GCP, or https.               \n",
              "\n",
              "Core Artifacts features                                                                                            \n",
              "\n",
              "W&amp;B Artifacts support dataset versioning through these basic features:                                             \n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "Let's now use embeddings with a vector database retriever to find relevant documents for a query\n"
      ],
      "metadata": {
        "id": "Qyimca-yW88x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# We will use the OpenAIEmbeddings to embed the text, and Chroma to store the vectors\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(document_sections, embeddings) # store the beddings"
      ],
      "metadata": {
        "id": "8kTAOL9AWrei"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with the embeddings we created, we now can retrieve from the database\n"
      ],
      "metadata": {
        "id": "v6QtheQSXYEI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_kwargs=dict(k=3))"
      ],
      "metadata": {
        "id": "be2L4Ao7YaL5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can I share my W&B report with my team members in a public W&B project?\"\n",
        "docs = retriever.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "C7DH8rwWYkVC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the results\n",
        "for doc in docs:\n",
        "  print(doc.metadata[\"source\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCiGDPtCYsdq",
        "outputId": "0b58d01a-dc59-43d1-f743-9375ecdffcd3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edu/llm-apps-course/docs_sample/collaborate-on-reports.md\n",
            "edu/llm-apps-course/docs_sample/collaborate-on-reports.md\n",
            "edu/llm-apps-course/docs_sample/teams.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stuff Prompt\n",
        "\n",
        "We will take the content of the retrieved documents, stuff them into prompt tempalte along with quiery, and pass it inot an LLM to obtain the answer"
      ],
      "metadata": {
        "id": "lyz-uBdAY2Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\",\"question\"]\n",
        ")\n",
        "\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "prompt = PROMPT.format(context=context, question = query)"
      ],
      "metadata": {
        "id": "Q8dAZMYQYxiB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use langchain to call OpenAI chat API with the question\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI()\n",
        "response = llm.predict(prompt)\n",
        "Markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "id": "fCGqgaZSbDt9",
        "outputId": "d57294a3-329e-40fe-b051-a3877d66e11d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "To share your W&B report with your team members in a public W&B project, you can select the Share button on the    \n",
              "upper right hand corner of the report. From there, you can either provide an email account or copy the magic link  \n",
              "to share with your team members. Users invited by email will need to log into Weights & Biases to view the report, \n",
              "while users who are given a magic link do not need to log in. It's important to note that shared reports are       \n",
              "view-only.                                                                                                         \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">To share your W&amp;B report with your team members in a public W&amp;B project, you can select the Share button on the    \n",
              "upper right hand corner of the report. From there, you can either provide an email account or copy the magic link  \n",
              "to share with your team members. Users invited by email will need to log into Weights &amp; Biases to view the report, \n",
              "while users who are given a magic link do not need to log in. It's important to note that shared reports are       \n",
              "view-only.                                                                                                         \n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Langchain\n",
        "\n",
        "langchain gives us tools to do this efficiently in few lines of code. Let's do the sameusing RetrievalQA chain"
      ],
      "metadata": {
        "id": "qdebqFmqbc0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type =\"stuff\", retriever=retriever)\n",
        "result = qa.run(query)\n",
        "\n",
        "Markdown(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "Fgy-gOUIbUe3",
        "outputId": "f8c56ca2-9cf0-4476-e2e2-a72ffe5ca849"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/khaihuyennguyen/llmapps/runs/sxf9t5hx\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "You can share your W&B report with your team members by selecting the Share button on the upper right hand corner  \n",
              "and either providing an email account or copying the magic link. Team members can also be invited to view the      \n",
              "report by email or by being given a magic link. Shared reports are view-only.                                      \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You can share your W&amp;B report with your team members by selecting the Share button on the upper right hand corner  \n",
              "and either providing an email account or copying the magic link. Team members can also be invited to view the      \n",
              "report by email or by being given a magic link. Shared reports are view-only.                                      \n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6qE4NkC0b0rT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlAkqX28cFxS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}