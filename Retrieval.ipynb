{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF9bPOsg+5OQ4URZvQm36o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khaihuyennguyen/NLP_LLM/blob/main/Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Retrieval Question Answering\n",
        "\n",
        "## Set up\n"
      ],
      "metadata": {
        "id": "4Q5_QhySgH_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T95ZaINogFf9"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq rich openai==0.27.2 tiktoken wandb langchain unstructured tabulate pdf2image chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "from pathlib import PurePosixPath\n",
        "import tiktoken\n",
        "from getpass import getpass\n",
        "from rich.markdown import Markdown"
      ],
      "metadata": {
        "id": "9eBv2zUcgRd4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "230Tn3iqgp8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getenv(\"OPENAI_API_KEY\") = None\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
        "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
        "print(\"OpenAI API key configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "ZGrr_qN0gayO",
        "outputId": "7ec95e88-d208-41b5-c89b-69325767c3af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-e067089e831f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    os.getenv(\"OPENAI_API_KEY\") = None\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R0qMP2NYFx4",
        "outputId": "a9827c55-eb43-4d76-ffda-7ca019979177"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI key from: https://platform.openai.com/account/api-keys\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "\n",
        "LangChain is a framework for developing application powered by langauge models. We will use some of its features in the code below. Let's start by configuring W&B tracking"
      ],
      "metadata": {
        "id": "Ctr90hnAgthm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need a single line of code to start tracing langchain with W&B\n",
        "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
        "\n",
        "# wandb documentation to configure wandb using env variables\n",
        "# https://docs.wandb.ai/guides/track/advanced/environment-variables\n",
        "# here we are configuring the wandb project name\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = \"llmapps\""
      ],
      "metadata": {
        "id": "A3VcEKc1gizI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing documents\n",
        "\n",
        "We will use a small sample of markdown documents in this notebook. Let's find them and make sure we can stuff them into the prompt. That means we may need to be chnked and not exceed some number of tokens"
      ],
      "metadata": {
        "id": "UNcx6lPoUZkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"text-davinci-003\"\n"
      ],
      "metadata": {
        "id": "bwo-n2dsUYRW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab some sample data,\n",
        "\n",
        "!git clone https://github.com/wandb/edu.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkv4zJrdUp33",
        "outputId": "4f451bef-d623-4332-daff-42d15f0d42a8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'edu' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need to count tokens in the documents , and for that we need the tokenizer\n",
        "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)"
      ],
      "metadata": {
        "id": "esqk0i3bUthM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "def find_md_files(directory):\n",
        "  \"Find all markdown files in a directory and return a LangChain documents\"\n",
        "  dl = DirectoryLoader(directory, \"**/*.md\")\n",
        "  return dl.load()\n",
        "\n",
        "documents = find_md_files('edu/llm-apps-course/docs_sample/')\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9_nWI5VVZg",
        "outputId": "d889efac-41b9-41fb-f73f-8024ee8dec93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The function to count the tokens in each doc\n",
        "def count_tokens(documents):\n",
        "  token_counts = [len(tokenizer.encode(document.page_content)) for document in documents]\n",
        "  return token_counts\n",
        "count_tokens(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ8FoRVyU3_u",
        "outputId": "8afb424c-6698-4226-db7f-53bb6ae4df3a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2135, 395, 763, 310, 665, 1957, 1154, 1199, 2657, 2676, 2330]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use LangChain built in Markdowntextsplitter to split the documents into sections.\n",
        "\n",
        "- We can pass the chunk_size param and avoid lengthy chunks\n",
        "- The chunk_overlap param is usefu so you dont cut sentences randomly. This is less necessary with Markdown"
      ],
      "metadata": {
        "id": "RPSIi4POV6Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "md_text_splitter = MarkdownTextSplitter(chunk_size = 1000)\n",
        "document_sections = md_text_splitter.split_documents(documents)\n",
        "len(document_sections), max(count_tokens(document_sections))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYw__CvdVMbd",
        "outputId": "797ca7d3-5186-48c6-9d28-b907b648899e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88, 438)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let see the first section\n",
        "Markdown(document_sections[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "SdHPfa7FWi3m",
        "outputId": "d640b6c8-0a5d-4c5b-d9a0-597c546deec7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "description: Guide to using Artifacts for dataset versioning                                                       \n",
              "\n",
              "Dataset Versioning                                                                                                 \n",
              "\n",
              "W&B Artifacts help you save and organize machine learning datasets throughout a project's lifecycle.               \n",
              "\n",
              "Common use cases                                                                                                   \n",
              "\n",
              "Version data seamlessly, without interrupting your workflow                                                        \n",
              "\n",
              "Prepackage data splits, like training, validation, and test sets                                                   \n",
              "\n",
              "Iteratively refine datasets, without desynchronizing the team                                                      \n",
              "\n",
              "Juggle multiple datasets, as in fine-tuning and domain adaptation                                                  \n",
              "\n",
              "Visualize & share your data workflow, keeping all your work in one place                                           \n",
              "\n",
              "Flexible tracking and hosting                                                                                      \n",
              "\n",
              "Beyond these common scenarios, you can use core Artifact features to upload, version, alias, compare, and download \n",
              "data, supporting any custom dataset workflow on local or remote file systems, via S3, GCP, or https.               \n",
              "\n",
              "Core Artifacts features                                                                                            \n",
              "\n",
              "W&B Artifacts support dataset versioning through these basic features:                                             \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">description: Guide to using Artifacts for dataset versioning                                                       \n",
              "\n",
              "Dataset Versioning                                                                                                 \n",
              "\n",
              "W&amp;B Artifacts help you save and organize machine learning datasets throughout a project's lifecycle.               \n",
              "\n",
              "Common use cases                                                                                                   \n",
              "\n",
              "Version data seamlessly, without interrupting your workflow                                                        \n",
              "\n",
              "Prepackage data splits, like training, validation, and test sets                                                   \n",
              "\n",
              "Iteratively refine datasets, without desynchronizing the team                                                      \n",
              "\n",
              "Juggle multiple datasets, as in fine-tuning and domain adaptation                                                  \n",
              "\n",
              "Visualize &amp; share your data workflow, keeping all your work in one place                                           \n",
              "\n",
              "Flexible tracking and hosting                                                                                      \n",
              "\n",
              "Beyond these common scenarios, you can use core Artifact features to upload, version, alias, compare, and download \n",
              "data, supporting any custom dataset workflow on local or remote file systems, via S3, GCP, or https.               \n",
              "\n",
              "Core Artifacts features                                                                                            \n",
              "\n",
              "W&amp;B Artifacts support dataset versioning through these basic features:                                             \n",
              "</pre>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "Let's now use embeddings with a vector database retriever to find relevant documents for a query\n"
      ],
      "metadata": {
        "id": "Qyimca-yW88x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# We will use the OpenAIEmbeddings to embed the text, and Chroma to store the vectors\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(document_sections, embeddings) # store the beddings"
      ],
      "metadata": {
        "id": "8kTAOL9AWrei"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with the embeddings we created, we now can retrieve from the database\n"
      ],
      "metadata": {
        "id": "v6QtheQSXYEI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_kwargs=dict(k=3))"
      ],
      "metadata": {
        "id": "be2L4Ao7YaL5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How can I share my W&B report with my team members in a public W&B project?\"\n",
        "docs = retriever.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "C7DH8rwWYkVC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the results\n",
        "for doc in docs:\n",
        "  print(doc.metadata[\"source\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCiGDPtCYsdq",
        "outputId": "0b58d01a-dc59-43d1-f743-9375ecdffcd3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edu/llm-apps-course/docs_sample/collaborate-on-reports.md\n",
            "edu/llm-apps-course/docs_sample/collaborate-on-reports.md\n",
            "edu/llm-apps-course/docs_sample/teams.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stuff Prompt\n",
        "\n",
        "We will take the content of the retrieved documents, stuff them into prompt tempalte along with quiery, and pass it inot an LLM to obtain the answer"
      ],
      "metadata": {
        "id": "lyz-uBdAY2Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8dAZMYQYxiB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}